## Portable LLama2

[en](./README_en.md) | [中文](./README_cn.md) 


本项目通过 wasm 技术将 LLama2 模型加载到浏览器中，实现web端的高效推理运算。  [在线demo](https://hku.github.io/pages/portable-llama2/)


### 模型训练

( 整理中 coming soon ... ）

### wasm 编译

( 整理中 coming soon ... ）


### 客户端安装

1. [安装node环境](https://nodejs.org)

2. 安装node包 ```npm install```

3. 运行web服务 ```npm run start```

4. 根据提示，打开网址： http://localhost:3000/


### 相关资源

[模型下载](https://huggingface.co/rayvvv/yumchat_cn) | [Gitee](https://gitee.com/hku2023/portable-llama2) | [Git] (https://github.com/hku/portable-llama2)


### 其它

在垂直细分场景下，portable 模型在算力节省，响应提速，数据隐私等方面有着明显的优势，对 portable 模型技术方向感兴趣的朋友，欢迎一起交流 ~

wx：


<img alt ="qrcode" src="../client/assets/qrcode2.jpg" width="300" height="auto">






